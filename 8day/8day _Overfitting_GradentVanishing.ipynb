{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 과적합(overfitting)을 막는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 데이터에 모델이 과적합되는 현상은 모델의 성능을 떨어트리는 주요 이슈이다.\n",
    "- 모델이 과적합되면 훈련 데이터에 대한 정확도는 높을지라도, 새로운 데이터 즉, 검증 뎅터나 데스트 데이터에 대해서는 제대로 동작하지 않는다.\n",
    "- 이는 모델이 학습 데이터를 불필요할 정도로 과하게 암기하여 훈련 데이터에 포함된 노이즈까지 학습한 상태라고 해석할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 데이터 양 늘리기\n",
    "\n",
    "- 모델은 데이터의 양이 적을 경우, 해당 데이터의 특정 패턴이나 노이즈까지 쉽게 암기하게 되므로 과적합 현상이 발생할 확률이 늘어난다.\n",
    "- 때문에 데이터 양을 늘릴수록 모델은 데이터의 일반적인 패턴을 학습하여 과적합을 방지할 수 있다.\n",
    "- 만약, 데이터의 양이 적을 경우에는 의도적으로 기존의 데이터를 조금씩 변형하고 추가하여 데이터의 양을 늘리기도 하는데 이를 데이터 증식 또는 증가라고 한다.\n",
    "- 이미지의 경우에는 데이터 중식이 많이 사용되는데 이미지를 돌리거나 노이즈를 추가하고 일부분을 수정하는 등으로 데이터를 증식한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 모델의 복잡도 줄이기\n",
    "\n",
    "- 인공 신경망의 복잡도는 은닉층의 수나 매개 변수의 수 등으로 결정된다.\n",
    "- 과적합 현상이 포착되었을 때, 인공 신경망 모델에 대해서 할 수 있는 한가지 조치는 인공 신경망의 복잡도를 줄이는 것이다.\n",
    "   - 인공 신경망에서는 모델에 있는 매개 변수들의 수를 모델의 수용력이라고 하기도 한다.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 가중치 규제( Rgularization ) 적용하기\n",
    "\n",
    "- 복잡한 모델이 간단한 모델보다 과적합될 가능성이 높다. 그리고 간단한 모델은 적은 수의 매개 변수를 가진 모델을 말한다.\n",
    "- 복잡한 모델을 좀 더 간단하게 하는 방법으로 가중치 규제가 있다.\n",
    "   - L1 규제 : 가중치 W들의 절대값 합계를 비용 함수에 추가한다. L1 노름이라고도 한다.\n",
    "   - L2 규제 : 모든 가중치 W들의 제곱합을 비용 함수에 추가한다. L2 노름이라고도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1 규제는 기존의 비용 함수에 모든 가중치에 대해서 $\\lambda \\mid W \\mid$를 더한 값을 비용 함수로 하고, L2 규제는 기존의 비용 함수에 모든 가중치에 대해서 $\\frac{1}{2} \\lambda W^2$를 더한 값을 비용 함수로 한다.\n",
    "\n",
    "- $\\lambda$는 규제의 강도를 정하는 하이퍼파라미터이다.\n",
    "- $\\lambda$가 크다면 모델이 훈련 데이터에 대해서 적합한 매개 변수를 찾는 것보다 규제를 위해 추가된 항들을 작게 유지하는 것을 우선한다는 의미가 된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 두 식 모두 비용 함수를 최소화하기 위해서는 가중치 W들의 값이 작아져야 한다는 특징이 있다.\n",
    "- L1 규제를 예로 들면 L1 규제를 사용하면 비용 함수가 최소가 되게 하는 가중치와 편향을 찾는 동시에 가중치들의 절대값의 합도 최소가 되어야 한다.\n",
    "- 이렇게 되면, 가중치 W의 값들은 0 또는 0에 가까이 작아져야 하므로 어떤 특성들은 모델을 만들 때 거의 사용되지 않게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예를 들어 $H( x ) = W_1x_1 + W_2x_2 + W_3x_3 + W_4x_4$라는 수식이 있을 때 여기에 L1 규제를 사용하였떠니, $W_3$의 값이 0이 되었다고 하면, 이는 $W_3$ 특성은 사실 모델의 결과에 별 영향을 주지 못하는 특성임을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L2 규제는 L1 규제와는 달리 가중치들의 제곱을 최소화하므로 W의 값이 완전히 0이 되기보다는 0에 가까워지는 경향을 뛴다.\n",
    "- L1 규제는 어떤 특성들이 모델에 영향을 주고 있는지를 정확히 판단하고자 할 때 유용하다.\n",
    "- 만약 이런 판단이 필요없다면 L2 규제가 더 잘 작동하므로 L2 규제를 더 권장한다.\n",
    "- 인공 신경망에서는 L2 규제는 가중치 감쇠( weight decay )라고도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     - 때로 Regularization을 정규화로 번역하는 참고 도서가 있는데,  이는 정규화( Normalization )와 혼동될 수 있으므로 규제 또는 정형화라는 번역이 바람직하다.\n",
    "     - 인공 신경망에서 정규화( Normalization )라는 용어가 쓰이는 기법으로는 배치 정규화, 총 정규화 등이다.\n",
    "     - 참고 : 표준화( Standardization )\n",
    "        - 각 obsevation이 평균을 기준으로 어느 정도 떨어져 있는지를 나타낼 때 사용된다.\n",
    "        - 값의 스케일이 다른 두 개의 변수가 있을 때, 이 변수들의 스케일 차이를 제거해 주는 효과가 있다.\n",
    "        - 제로 평균으로부터 각 값들의 분산을 나타낸다.\n",
    "        - 각 요소의 값에서 평균을 뺀 다음 표준편차로 나누어 준다.\n",
    "    \n",
    "![Alt text]( standardization.png )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     - 참고 : 정규화( Normalization )\n",
    "        - 정규화는 데이터의 범위를 0과 1로 변환하여 데이터 분포를 조정하는 방법이다.\n",
    "        - ( 해당 값 - 최소값 ) / ( 최대값 - 최소값 )\n",
    "![Alt text]( normalization.png )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) 드롭 아웃( Dropout )\n",
    "\n",
    "- 드롭 아웃은 학습 과정에서 신경망의 일부를 사용하지 않는 방법이다.\n",
    "- 예를들어 드롭 아웃의 비율을 0.5로 한다면 학습 과정마다 랜덤으로 절반의 뉴런을 사용하지 않고, 절반의 뉴런만을 사용한다.\n",
    "\n",
    "![Alt text]( dropout.png )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 드롭 아웃은 신경망 학습 시에만 사용하고, 예측 시에는 사용하지 않는 것이 일반적이다.\n",
    "- 학습 시에 인공 신경망이 특정 뉴런 또는 특정 조합에 너무 의존적이게 되는 것을 방지해주고, 매번 랜덤 선택으로 뉴런들을 사용하지 않으므로 서로 다른 신경망들을 앙상블하여 사용하는 것 같은 효과를 내어 과적합을 방지한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b188c274110f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Keras를 드롭아웃 모델\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 드롭아웃 추가. 비율은 50%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_words' is not defined"
     ]
    }
   ],
   "source": [
    "# Keras를 드롭아웃 모델\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(max_words,), activation='relu'))\n",
    "model.add(Dropout(0.5)) # 드롭아웃 추가. 비율은 50%\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5)) # 드롭아웃 추가. 비율은 50%\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 기울기 소실(Gradient Vanishing)과 폭주(Exploding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 심층 신경망을 학습하다보면 역전파 과정에서 입력층으로 갈수록 기울기가 점차적으로 작아지는 현상이 발생할 수 있다.\n",
    "- 입력층에 가까운 층들에서 가중치들이 업데이트가 제대로 되지 않으면 결국 최적의 모델을 찾을 수 없게 된다. 이를 기울기 소실이라 한다.\n",
    "- 반대의 경우도 있는데, 기울기가 점차 증가하면서 가중치들이 비정상적으로 큰 값이 되면서 결국 발산되기도 한다. 이를 기울기 폭주( Gradient Vanishing )이라고 하며, 순환 신경망( Recurrent Neural Network Exploding )이라고 하며, 순환 신경망(RNN)에서 발생한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) ReLU와 ReLU 변형들\n",
    "\n",
    "- 시그모이드 함수를 사용하면 입력의 절대값이 클 경우에 시그모읻 함수의 출력값이 0 또는 1에 수렴하면서 기울기가 0에 가까워진다.\n",
    "- 그래서 역전파 과정에서 전파 시킬 기울기가 점차 사라져서 입력층 방향으로 갈 수록 제대로 역전파가 되지 않는 기울기 소실 문제가 발생한다.\n",
    "\n",
    "\n",
    "- 기우릭 소실을 완화하는 가장 간단한 방법은 은닉층의 활성화 함수로 시그모이드나 하이퍼볼릭탄젠트 함수 대신에 ReLU나 ReLU 변형 함수인 Leaky ReLU를 사용한다.\n",
    "   - 은닉층에서는 시그모이드 함수 사용을 지향한다.\n",
    "   - Leaky ReLU를 사용하면 모든 입력값에 대해서 기울기가 0에 수렴하지 앟아 죽은 ReLU 문제를 해결한다.\n",
    "   - 은닉층에서는 ReLU나 Leaky ReLU와 같은 ReLU 변형 함수를 사용한다.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 그래디언트 클리핑(Gradient Clipping)\n",
    "\n",
    "- 그래디언트 클리핑은 기울기 값을 자르는 것을 의미한다.\n",
    "- 기울기 폭주를 막기 위해 임계값을 넘지 않도록 값을 자른다. 즉, 임계치만큼 크기를 감소시킨다.\n",
    "- 그래디언트 클리핑은 순환 신경망( Recurrent Neural Network, RNN )에 유용하다. RNN은 BPTT에서 시점을 역행하면서 기울기를 구하는데, 이 때 기울기가 너무 커질 수 있기 때문이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras 그래디언트 클리핑\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "Adam = optimizers.Adam(lr=0.0001, clipnorm=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 가중치 초기화(Weight initialization)\n",
    "\n",
    "- 같은 모델을 훈련시키더라도 가중치가 초기에 어떤 값을 가졌느냐에 따라서 모델의 훈련 결과가 달라지기도 한다.\n",
    "- 즉, 가증치 초기화만 적절히 해줘도 기울기 소실 문제와 같은 문제를 완화시킬 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 세이비어 초기화(Xavier Initialization)\n",
    "\n",
    "- 논문 : http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
    "\n",
    "- 2010년 세이비어 글로럿과 요슈아 벤지오는 가중치 초기화가 모델에 미치는 영향을 분석하여 새로운 초기화 방법을 제안하였다.\n",
    "- 이 초기화 방법은 제안한 사람의 이름을 따서 세이비어(Xavier Initialization)초기화 또는 글로럿( Glorot Initialization )초기화라 한다.\n",
    "- 이 방법은 균등 분포( Uniform Distribution ) 또는 정규 분포( Normal distribution )로 초기화 할 때 두 가지 경우로 나뉘며, 이전 층의 뉴런 개수와 다음 층의 뉴런 개수를 가지고 식을 세운다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
